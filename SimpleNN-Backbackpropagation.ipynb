{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Original Author : Matt Mazur\n",
    "## https://github.com/mattm/simple-neural-network\n",
    "\n",
    "## Revised for bias updated. \n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "#\n",
    "# Shorthand:\n",
    "#   \"pd_\" as a variable prefix means \"partial derivative\"\n",
    "#   \"d_\" as a variable prefix means \"derivative\"\n",
    "#   \"_wrt_\" is shorthand for \"with respect to\"\n",
    "#   \"w_ho\" and \"w_ih\" are the index of weights from hidden to output layer neurons and input to hidden layer neurons respectively\n",
    "#\n",
    "# Comment references:\n",
    "#\n",
    "# [1] Wikipedia article on Backpropagation\n",
    "#   http://en.wikipedia.org/wiki/Backpropagation#Finding_the_derivative_of_the_error\n",
    "# [2] Neural Networks for Machine Learning course on Coursera by Geoffrey Hinton\n",
    "#   https://class.coursera.org/neuralnets-2012-001/lecture/39\n",
    "# [3] The Back Propagation Algorithm\n",
    "#   https://www4.rgu.ac.uk/files/chapter3%20-%20bp.pdf\n",
    "\n",
    "class NeuralNetwork:\n",
    "    LEARNING_RATE = 0.5\n",
    "\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights = None, hidden_layer_bias = None, output_layer_weights = None, output_layer_bias = None):\n",
    "        self.num_inputs = num_inputs\n",
    "\n",
    "        self.hidden_layer = NeuronLayer(num_hidden, hidden_layer_bias)\n",
    "        self.output_layer = NeuronLayer(num_outputs, output_layer_bias)\n",
    "\n",
    "        self.init_weights_from_inputs_to_hidden_layer_neurons(hidden_layer_weights)\n",
    "        self.init_weights_from_hidden_layer_neurons_to_output_layer_neurons(output_layer_weights)\n",
    "\n",
    "    def init_weights_from_inputs_to_hidden_layer_neurons(self, hidden_layer_weights):\n",
    "        weight_num = 0\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for i in range(self.num_inputs):\n",
    "                if not hidden_layer_weights:\n",
    "                    self.hidden_layer.neurons[h].weights.append(random.random())\n",
    "                else:\n",
    "                    self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])\n",
    "                weight_num += 1\n",
    "\n",
    "    def init_weights_from_hidden_layer_neurons_to_output_layer_neurons(self, output_layer_weights):\n",
    "        weight_num = 0\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for h in range(len(self.hidden_layer.neurons)):\n",
    "                if not output_layer_weights:\n",
    "                    self.output_layer.neurons[o].weights.append(random.random())\n",
    "                else:\n",
    "                    self.output_layer.neurons[o].weights.append(output_layer_weights[weight_num])\n",
    "                weight_num += 1\n",
    "\n",
    "    def inspect(self):\n",
    "        print('------')\n",
    "        print('* Inputs: {}'.format(self.num_inputs))\n",
    "        print('------')\n",
    "        print('Hidden Layer')\n",
    "        self.hidden_layer.inspect()\n",
    "        print('------')\n",
    "        print('* Output Layer')\n",
    "        self.output_layer.inspect()\n",
    "        print('------')\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        hidden_layer_outputs = self.hidden_layer.feed_forward(inputs)\n",
    "        return self.output_layer.feed_forward(hidden_layer_outputs)\n",
    "\n",
    "    # Uses online learning, ie updating the weights after each training case\n",
    "    def train(self, training_inputs, training_outputs):\n",
    "        self.feed_forward(training_inputs)\n",
    "\n",
    "        # 1. Output neuron deltas\n",
    "        pd_errors_wrt_output_neuron_total_net_input = [0] * len(self.output_layer.neurons)\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "\n",
    "            # ∂E/∂zⱼ\n",
    "            pd_errors_wrt_output_neuron_total_net_input[o] = self.output_layer.neurons[o].calculate_pd_error_wrt_total_net_input(training_outputs[o])\n",
    "\n",
    "        # 2. Hidden neuron deltas\n",
    "        pd_errors_wrt_hidden_neuron_total_net_input = [0] * len(self.hidden_layer.neurons)\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "\n",
    "            # We need to calculate the derivative of the error with respect to the output of each hidden layer neuron\n",
    "            # dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ\n",
    "            d_error_wrt_hidden_neuron_output = 0\n",
    "            for o in range(len(self.output_layer.neurons)):\n",
    "                d_error_wrt_hidden_neuron_output += pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].weights[h]\n",
    "\n",
    "            # ∂E/∂zⱼ = dE/dyⱼ * ∂zⱼ/∂\n",
    "            pd_errors_wrt_hidden_neuron_total_net_input[h] = d_error_wrt_hidden_neuron_output * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_input()\n",
    "\n",
    "        # 3. Update output neuron weights\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for w_ho in range(len(self.output_layer.neurons[o].weights)):\n",
    "\n",
    "                # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ\n",
    "                pd_error_wrt_weight = pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].calculate_pd_total_net_input_wrt_weight(w_ho)\n",
    "\n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.output_layer.neurons[o].weights[w_ho] -= self.LEARNING_RATE * pd_error_wrt_weight\n",
    "\n",
    "        # 4. Update hidden neuron weights\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for w_ih in range(len(self.hidden_layer.neurons[h].weights)):\n",
    "\n",
    "                # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ\n",
    "                pd_error_wrt_weight = pd_errors_wrt_hidden_neuron_total_net_input[h] * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_weight(w_ih)\n",
    "\n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.hidden_layer.neurons[h].weights[w_ih] -= self.LEARNING_RATE * pd_error_wrt_weight\n",
    "\n",
    "        # 5. Update output neuron bias\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "                # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ\n",
    "                pd_error_wrt_weight = pd_errors_wrt_output_neuron_total_net_input[o] *  \\\n",
    "                                      self.output_layer.neurons[o].calculate_pd_total_net_input_bias()\n",
    "\n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.output_layer.neurons[o].bias -= self.LEARNING_RATE * pd_error_wrt_weight        \n",
    "        \n",
    "        # 6. Update hidden neuron weights\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "                # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ\n",
    "                pd_error_wrt_weight = pd_errors_wrt_hidden_neuron_total_net_input[h] *  \\\n",
    "                                      self.hidden_layer.neurons[h].calculate_pd_total_net_input_bias()\n",
    "\n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.hidden_layer.neurons[h].bias -= self.LEARNING_RATE * pd_error_wrt_weight\n",
    "        \n",
    "    def calculate_total_error(self, training_sets):\n",
    "        total_error = 0\n",
    "        for t in range(len(training_sets)):\n",
    "            training_inputs, training_outputs = training_sets[t]\n",
    "            self.feed_forward(training_inputs)\n",
    "            for o in range(len(training_outputs)):\n",
    "                total_error += self.output_layer.neurons[o].calculate_error(training_outputs[o])\n",
    "        return total_error\n",
    "\n",
    "class NeuronLayer:\n",
    "    def __init__(self, num_neurons, bias):\n",
    "\n",
    "        # Every neuron in a layer shares the same bias\n",
    "        self.bias = bias if bias else random.random()\n",
    "\n",
    "        self.neurons = []\n",
    "        for i in range(num_neurons):\n",
    "            self.neurons.append(Neuron(self.bias))\n",
    "\n",
    "    #\n",
    "    # one bias value per neuron\n",
    "    def inspect(self):\n",
    "        print('Neurons:', len(self.neurons))\n",
    "        for n in range(len(self.neurons)):\n",
    "            print(' Neuron', n)\n",
    "            for w in range(len(self.neurons[n].weights)):\n",
    "                print('  Weight:', self.neurons[n].weights[w])\n",
    "            print('  Bias:', self.neurons[0].bias)\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.calculate_output(inputs))\n",
    "        return outputs\n",
    "\n",
    "    def get_outputs(self):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.output)\n",
    "        return outputs\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, bias):\n",
    "        self.bias = bias\n",
    "        self.weights = []\n",
    "\n",
    "    def calculate_output(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = self.squash(self.calculate_total_net_input())\n",
    "        return self.output\n",
    "\n",
    "    def calculate_total_net_input(self):\n",
    "        total = 0\n",
    "        for i in range(len(self.inputs)):\n",
    "            total += self.inputs[i] * self.weights[i]\n",
    "        return total + self.bias\n",
    "\n",
    "    # Apply the logistic function to squash the output of the neuron\n",
    "    # The result is sometimes referred to as 'net' [2] or 'net' [1]\n",
    "    def squash(self, total_net_input):\n",
    "        return 1 / (1 + math.exp(-total_net_input))\n",
    "\n",
    "    # Determine how much the neuron's total input has to change to move closer to the expected output\n",
    "    #\n",
    "    # Now that we have the partial derivative of the error with respect to the output (∂E/∂yⱼ) and\n",
    "    # the derivative of the output with respect to the total net input (dyⱼ/dzⱼ) we can calculate\n",
    "    # the partial derivative of the error with respect to the total net input.\n",
    "    # This value is also known as the delta (δ) [1]\n",
    "    # δ = ∂E/∂zⱼ = ∂E/∂yⱼ * dyⱼ/dzⱼ\n",
    "    #\n",
    "    def calculate_pd_error_wrt_total_net_input(self, target_output):\n",
    "        return self.calculate_pd_error_wrt_output(target_output) * self.calculate_pd_total_net_input_wrt_input();\n",
    "\n",
    "    # The error for each neuron is calculated by the Mean Square Error method:\n",
    "    def calculate_error(self, target_output):\n",
    "        return 0.5 * (target_output - self.output) ** 2\n",
    "\n",
    "    # The partial derivate of the error with respect to actual output then is calculated by:\n",
    "    # = 2 * 0.5 * (target output - actual output) ^ (2 - 1) * -1\n",
    "    # = -(target output - actual output)\n",
    "    #\n",
    "    # The Wikipedia article on backpropagation [1] simplifies to the following, but most other learning material does not [2]\n",
    "    # = actual output - target output\n",
    "    #\n",
    "    # Alternative, you can use (target - output), but then need to add it during backpropagation [3]\n",
    "    #\n",
    "    # Note that the actual output of the output neuron is often written as yⱼ and target output as tⱼ so:\n",
    "    # = ∂E/∂yⱼ = -(tⱼ - yⱼ)\n",
    "    def calculate_pd_error_wrt_output(self, target_output):\n",
    "        return -(target_output - self.output)\n",
    "\n",
    "    # The total net input into the neuron is squashed using logistic function to calculate the neuron's output:\n",
    "    # yⱼ = φ = 1 / (1 + e^(-zⱼ))\n",
    "    # Note that where ⱼ represents the output of the neurons in whatever layer we're looking at and ᵢ represents the layer below it\n",
    "    #\n",
    "    # The derivative (not partial derivative since there is only one variable) of the output then is:\n",
    "    # dyⱼ/dzⱼ = yⱼ * (1 - yⱼ)\n",
    "    def calculate_pd_total_net_input_wrt_input(self):\n",
    "        return self.output * (1 - self.output)\n",
    "\n",
    "    # The total net input is the weighted sum of all the inputs to the neuron and their respective weights:\n",
    "    # = zⱼ = netⱼ = x₁w₁ + x₂w₂ ... + b\n",
    "    #\n",
    "    # The partial derivative of the total net input with respective to a given weight (with everything else held constant) then is:\n",
    "    # = ∂zⱼ/∂wᵢ = some constant + 1 * xᵢw₁^(1-0) + some constant ... = xᵢ\n",
    "    def calculate_pd_total_net_input_wrt_weight(self, index):\n",
    "        return self.inputs[index]\n",
    "    \n",
    "    def calculate_pd_total_net_input_bias(self):\n",
    "        return 1\n",
    "\n",
    "###\n",
    "\n",
    "# Blog post example:\n",
    "\n",
    "\n",
    "# XOR example:\n",
    "\n",
    "# training_sets = [\n",
    "#     [[0, 0], [0]],\n",
    "#     [[0, 1], [1]],\n",
    "#     [[1, 0], [1]],\n",
    "#     [[1, 1], [0]]\n",
    "# ]\n",
    "\n",
    "# nn = NeuralNetwork(len(training_sets[0][0]), 5, len(training_sets[0][1]))\n",
    "# for i in range(10000):\n",
    "#     training_inputs, training_outputs = random.choice(training_sets)\n",
    "#     nn.train(training_inputs, training_outputs)\n",
    "#     print(i, nn.calculate_total_error(training_sets))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "* Inputs: 2\n",
      "------\n",
      "Hidden Layer\n",
      "('Neurons:', 2)\n",
      "(' Neuron', 0)\n",
      "('  Weight:', 0.15)\n",
      "('  Weight:', 0.2)\n",
      "('  Bias:', 0.35)\n",
      "(' Neuron', 1)\n",
      "('  Weight:', 0.25)\n",
      "('  Weight:', 0.3)\n",
      "('  Bias:', 0.35)\n",
      "------\n",
      "* Output Layer\n",
      "('Neurons:', 2)\n",
      "(' Neuron', 0)\n",
      "('  Weight:', 0.4)\n",
      "('  Weight:', 0.45)\n",
      "('  Bias:', 0.6)\n",
      "(' Neuron', 1)\n",
      "('  Weight:', 0.5)\n",
      "('  Weight:', 0.55)\n",
      "('  Bias:', 0.6)\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(2, 2, 2, hidden_layer_weights=[0.15, 0.2, 0.25, 0.3], hidden_layer_bias=0.35, output_layer_weights=[0.4, 0.45, 0.5, 0.55], output_layer_bias=0.6)\n",
    "nn.inspect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.113602735)\n",
      "(1, 0.103014893)\n",
      "(2, 0.093663058)\n",
      "(3, 0.085427754)\n",
      "(4, 0.078183997)\n",
      "(5, 0.07181033)\n",
      "(6, 0.066194128)\n",
      "(7, 0.061234149)\n",
      "(8, 0.056841263)\n",
      "(9, 0.052938092)\n",
      "(10, 0.04945807)\n",
      "(11, 0.046344249)\n",
      "(12, 0.043548059)\n",
      "(13, 0.041028127)\n",
      "(14, 0.038749202)\n",
      "(15, 0.036681218)\n",
      "(16, 0.034798481)\n",
      "(17, 0.033078983)\n",
      "(18, 0.031503823)\n",
      "(19, 0.03005672)\n",
      "(20, 0.028723603)\n",
      "(21, 0.027492278)\n",
      "(22, 0.026352138)\n",
      "(23, 0.025293932)\n",
      "(24, 0.02430956)\n",
      "(25, 0.023391912)\n",
      "(26, 0.022534727)\n",
      "(27, 0.021732474)\n",
      "(28, 0.02098025)\n",
      "(29, 0.020273704)\n",
      "(30, 0.019608955)\n",
      "(31, 0.018982539)\n",
      "(32, 0.018391352)\n",
      "(33, 0.017832608)\n",
      "(34, 0.017303799)\n",
      "(35, 0.016802663)\n",
      "(36, 0.016327154)\n",
      "(37, 0.01587542)\n",
      "(38, 0.015445776)\n",
      "(39, 0.015036693)\n",
      "(40, 0.014646775)\n",
      "(41, 0.014274747)\n",
      "(42, 0.013919441)\n",
      "(43, 0.013579791)\n",
      "(44, 0.013254813)\n",
      "(45, 0.012943606)\n",
      "(46, 0.012645338)\n",
      "(47, 0.012359242)\n",
      "(48, 0.012084609)\n",
      "(49, 0.011820785)\n",
      "(50, 0.011567162)\n",
      "(51, 0.011323176)\n",
      "(52, 0.011088305)\n",
      "(53, 0.010862061)\n",
      "(54, 0.010643992)\n",
      "(55, 0.010433676)\n",
      "(56, 0.010230717)\n",
      "(57, 0.010034748)\n",
      "(58, 0.009845424)\n",
      "(59, 0.009662423)\n",
      "(60, 0.009485442)\n",
      "(61, 0.009314198)\n",
      "(62, 0.009148424)\n",
      "(63, 0.00898787)\n",
      "(64, 0.008832301)\n",
      "(65, 0.008681494)\n",
      "(66, 0.008535241)\n",
      "(67, 0.008393345)\n",
      "(68, 0.008255619)\n",
      "(69, 0.008121888)\n",
      "(70, 0.007991985)\n",
      "(71, 0.007865754)\n",
      "(72, 0.007743046)\n",
      "(73, 0.00762372)\n",
      "(74, 0.007507641)\n",
      "(75, 0.007394684)\n",
      "(76, 0.007284727)\n",
      "(77, 0.007177657)\n",
      "(78, 0.007073365)\n",
      "(79, 0.006971747)\n",
      "(80, 0.006872705)\n",
      "(81, 0.006776145)\n",
      "(82, 0.006681978)\n",
      "(83, 0.006590118)\n",
      "(84, 0.006500486)\n",
      "(85, 0.006413003)\n",
      "(86, 0.006327595)\n",
      "(87, 0.006244191)\n",
      "(88, 0.006162725)\n",
      "(89, 0.006083131)\n",
      "(90, 0.006005348)\n",
      "(91, 0.005929316)\n",
      "(92, 0.00585498)\n",
      "(93, 0.005782284)\n",
      "(94, 0.005711177)\n",
      "(95, 0.005641609)\n",
      "(96, 0.005573532)\n",
      "(97, 0.0055069)\n",
      "(98, 0.005441669)\n",
      "(99, 0.005377797)\n",
      "(100, 0.005315244)\n",
      "(101, 0.00525397)\n",
      "(102, 0.005193937)\n",
      "(103, 0.00513511)\n",
      "(104, 0.005077453)\n",
      "(105, 0.005020934)\n",
      "(106, 0.00496552)\n",
      "(107, 0.00491118)\n",
      "(108, 0.004857884)\n",
      "(109, 0.004805604)\n",
      "(110, 0.004754311)\n",
      "(111, 0.004703979)\n",
      "(112, 0.004654582)\n",
      "(113, 0.004606095)\n",
      "(114, 0.004558495)\n",
      "(115, 0.004511757)\n",
      "(116, 0.004465859)\n",
      "(117, 0.004420779)\n",
      "(118, 0.004376498)\n",
      "(119, 0.004332994)\n",
      "(120, 0.004290247)\n",
      "(121, 0.00424824)\n",
      "(122, 0.004206953)\n",
      "(123, 0.004166369)\n",
      "(124, 0.00412647)\n",
      "(125, 0.00408724)\n",
      "(126, 0.004048664)\n",
      "(127, 0.004010724)\n",
      "(128, 0.003973407)\n",
      "(129, 0.003936697)\n",
      "(130, 0.00390058)\n",
      "(131, 0.003865043)\n",
      "(132, 0.003830073)\n",
      "(133, 0.003795655)\n",
      "(134, 0.003761778)\n",
      "(135, 0.00372843)\n",
      "(136, 0.003695598)\n",
      "(137, 0.003663272)\n",
      "(138, 0.003631439)\n",
      "(139, 0.00360009)\n",
      "(140, 0.003569213)\n",
      "(141, 0.003538799)\n",
      "(142, 0.003508837)\n",
      "(143, 0.003479319)\n",
      "(144, 0.003450233)\n",
      "(145, 0.003421572)\n",
      "(146, 0.003393326)\n",
      "(147, 0.003365487)\n",
      "(148, 0.003338046)\n",
      "(149, 0.003310996)\n",
      "(150, 0.003284328)\n",
      "(151, 0.003258034)\n",
      "(152, 0.003232107)\n",
      "(153, 0.00320654)\n",
      "(154, 0.003181325)\n",
      "(155, 0.003156455)\n",
      "(156, 0.003131924)\n",
      "(157, 0.003107725)\n",
      "(158, 0.003083852)\n",
      "(159, 0.003060298)\n",
      "(160, 0.003037058)\n",
      "(161, 0.003014124)\n",
      "(162, 0.002991492)\n",
      "(163, 0.002969155)\n",
      "(164, 0.002947109)\n",
      "(165, 0.002925347)\n",
      "(166, 0.002903865)\n",
      "(167, 0.002882657)\n",
      "(168, 0.002861718)\n",
      "(169, 0.002841044)\n",
      "(170, 0.002820629)\n",
      "(171, 0.00280047)\n",
      "(172, 0.00278056)\n",
      "(173, 0.002760897)\n",
      "(174, 0.002741475)\n",
      "(175, 0.00272229)\n",
      "(176, 0.002703338)\n",
      "(177, 0.002684616)\n",
      "(178, 0.002666118)\n",
      "(179, 0.002647842)\n",
      "(180, 0.002629783)\n",
      "(181, 0.002611937)\n",
      "(182, 0.002594302)\n",
      "(183, 0.002576873)\n",
      "(184, 0.002559648)\n",
      "(185, 0.002542622)\n",
      "(186, 0.002525792)\n",
      "(187, 0.002509155)\n",
      "(188, 0.002492709)\n",
      "(189, 0.002476449)\n",
      "(190, 0.002460372)\n",
      "(191, 0.002444477)\n",
      "(192, 0.002428759)\n",
      "(193, 0.002413217)\n",
      "(194, 0.002397846)\n",
      "(195, 0.002382646)\n",
      "(196, 0.002367611)\n",
      "(197, 0.002352741)\n",
      "(198, 0.002338033)\n",
      "(199, 0.002323484)\n",
      "(200, 0.002309091)\n",
      "(201, 0.002294853)\n",
      "(202, 0.002280766)\n",
      "(203, 0.002266829)\n",
      "(204, 0.002253039)\n",
      "(205, 0.002239394)\n",
      "(206, 0.002225892)\n",
      "(207, 0.00221253)\n",
      "(208, 0.002199307)\n",
      "(209, 0.002186221)\n",
      "(210, 0.002173269)\n",
      "(211, 0.002160449)\n",
      "(212, 0.002147761)\n",
      "(213, 0.0021352)\n",
      "(214, 0.002122767)\n",
      "(215, 0.002110458)\n",
      "(216, 0.002098273)\n",
      "(217, 0.002086209)\n",
      "(218, 0.002074264)\n",
      "(219, 0.002062438)\n",
      "(220, 0.002050728)\n",
      "(221, 0.002039132)\n",
      "(222, 0.00202765)\n",
      "(223, 0.002016279)\n",
      "(224, 0.002005017)\n",
      "(225, 0.001993865)\n",
      "(226, 0.001982819)\n",
      "(227, 0.001971878)\n",
      "(228, 0.001961042)\n",
      "(229, 0.001950308)\n",
      "(230, 0.001939675)\n",
      "(231, 0.001929142)\n",
      "(232, 0.001918707)\n",
      "(233, 0.00190837)\n",
      "(234, 0.001898128)\n",
      "(235, 0.001887981)\n",
      "(236, 0.001877927)\n",
      "(237, 0.001867965)\n",
      "(238, 0.001858094)\n",
      "(239, 0.001848313)\n",
      "(240, 0.001838621)\n",
      "(241, 0.001829015)\n",
      "(242, 0.001819496)\n",
      "(243, 0.001810062)\n",
      "(244, 0.001800712)\n",
      "(245, 0.001791445)\n",
      "(246, 0.00178226)\n",
      "(247, 0.001773156)\n",
      "(248, 0.001764132)\n",
      "(249, 0.001755187)\n",
      "(250, 0.001746319)\n",
      "(251, 0.001737528)\n",
      "(252, 0.001728813)\n",
      "(253, 0.001720174)\n",
      "(254, 0.001711608)\n",
      "(255, 0.001703115)\n",
      "(256, 0.001694695)\n",
      "(257, 0.001686346)\n",
      "(258, 0.001678068)\n",
      "(259, 0.001669859)\n",
      "(260, 0.001661719)\n",
      "(261, 0.001653647)\n",
      "(262, 0.001645643)\n",
      "(263, 0.001637704)\n",
      "(264, 0.001629832)\n",
      "(265, 0.001622024)\n",
      "(266, 0.00161428)\n",
      "(267, 0.001606599)\n",
      "(268, 0.001598981)\n",
      "(269, 0.001591425)\n",
      "(270, 0.00158393)\n",
      "(271, 0.001576495)\n",
      "(272, 0.00156912)\n",
      "(273, 0.001561804)\n",
      "(274, 0.001554547)\n",
      "(275, 0.001547347)\n",
      "(276, 0.001540204)\n",
      "(277, 0.001533117)\n",
      "(278, 0.001526086)\n",
      "(279, 0.001519111)\n",
      "(280, 0.001512189)\n",
      "(281, 0.001505322)\n",
      "(282, 0.001498508)\n",
      "(283, 0.001491746)\n",
      "(284, 0.001485037)\n",
      "(285, 0.001478379)\n",
      "(286, 0.001471773)\n",
      "(287, 0.001465216)\n",
      "(288, 0.00145871)\n",
      "(289, 0.001452252)\n",
      "(290, 0.001445844)\n",
      "(291, 0.001439484)\n",
      "(292, 0.001433171)\n",
      "(293, 0.001426906)\n",
      "(294, 0.001420687)\n",
      "(295, 0.001414515)\n",
      "(296, 0.001408388)\n",
      "(297, 0.001402307)\n",
      "(298, 0.00139627)\n",
      "(299, 0.001390278)\n",
      "(300, 0.001384329)\n",
      "(301, 0.001378424)\n",
      "(302, 0.001372562)\n",
      "(303, 0.001366742)\n",
      "(304, 0.001360964)\n",
      "(305, 0.001355228)\n",
      "(306, 0.001349533)\n",
      "(307, 0.001343879)\n",
      "(308, 0.001338265)\n",
      "(309, 0.001332691)\n",
      "(310, 0.001327156)\n",
      "(311, 0.00132166)\n",
      "(312, 0.001316204)\n",
      "(313, 0.001310785)\n",
      "(314, 0.001305405)\n",
      "(315, 0.001300062)\n",
      "(316, 0.001294756)\n",
      "(317, 0.001289487)\n",
      "(318, 0.001284254)\n",
      "(319, 0.001279058)\n",
      "(320, 0.001273897)\n",
      "(321, 0.001268772)\n",
      "(322, 0.001263681)\n",
      "(323, 0.001258625)\n",
      "(324, 0.001253604)\n",
      "(325, 0.001248617)\n",
      "(326, 0.001243663)\n",
      "(327, 0.001238743)\n",
      "(328, 0.001233855)\n",
      "(329, 0.001229001)\n",
      "(330, 0.001224179)\n",
      "(331, 0.001219389)\n",
      "(332, 0.00121463)\n",
      "(333, 0.001209904)\n",
      "(334, 0.001205208)\n",
      "(335, 0.001200543)\n",
      "(336, 0.001195909)\n",
      "(337, 0.001191306)\n",
      "(338, 0.001186732)\n",
      "(339, 0.001182188)\n",
      "(340, 0.001177673)\n",
      "(341, 0.001173188)\n",
      "(342, 0.001168732)\n",
      "(343, 0.001164304)\n",
      "(344, 0.001159905)\n",
      "(345, 0.001155533)\n",
      "(346, 0.00115119)\n",
      "(347, 0.001146874)\n",
      "(348, 0.001142586)\n",
      "(349, 0.001138325)\n",
      "(350, 0.001134091)\n",
      "(351, 0.001129883)\n",
      "(352, 0.001125702)\n",
      "(353, 0.001121547)\n",
      "(354, 0.001117418)\n",
      "(355, 0.001113314)\n",
      "(356, 0.001109236)\n",
      "(357, 0.001105184)\n",
      "(358, 0.001101156)\n",
      "(359, 0.001097153)\n",
      "(360, 0.001093175)\n",
      "(361, 0.001089221)\n",
      "(362, 0.001085291)\n",
      "(363, 0.001081385)\n",
      "(364, 0.001077503)\n",
      "(365, 0.001073645)\n",
      "(366, 0.001069809)\n",
      "(367, 0.001065997)\n",
      "(368, 0.001062208)\n",
      "(369, 0.001058441)\n",
      "(370, 0.001054698)\n",
      "(371, 0.001050976)\n",
      "(372, 0.001047276)\n",
      "(373, 0.001043599)\n",
      "(374, 0.001039943)\n",
      "(375, 0.001036309)\n",
      "(376, 0.001032696)\n",
      "(377, 0.001029105)\n",
      "(378, 0.001025534)\n",
      "(379, 0.001021985)\n",
      "(380, 0.001018456)\n",
      "(381, 0.001014948)\n",
      "(382, 0.00101146)\n",
      "(383, 0.001007992)\n",
      "(384, 0.001004544)\n",
      "(385, 0.001001116)\n",
      "(386, 0.000997708)\n",
      "(387, 0.000994319)\n",
      "(388, 0.00099095)\n",
      "(389, 0.0009876)\n",
      "(390, 0.000984269)\n",
      "(391, 0.000980956)\n",
      "(392, 0.000977663)\n",
      "(393, 0.000974388)\n",
      "(394, 0.000971132)\n",
      "(395, 0.000967894)\n",
      "(396, 0.000964674)\n",
      "(397, 0.000961472)\n",
      "(398, 0.000958288)\n",
      "(399, 0.000955121)\n",
      "(400, 0.000951973)\n",
      "(401, 0.000948841)\n",
      "(402, 0.000945727)\n",
      "(403, 0.00094263)\n",
      "(404, 0.00093955)\n",
      "(405, 0.000936488)\n",
      "(406, 0.000933441)\n",
      "(407, 0.000930412)\n",
      "(408, 0.000927399)\n",
      "(409, 0.000924402)\n",
      "(410, 0.000921422)\n",
      "(411, 0.000918457)\n",
      "(412, 0.000915509)\n",
      "(413, 0.000912577)\n",
      "(414, 0.00090966)\n",
      "(415, 0.000906759)\n",
      "(416, 0.000903873)\n",
      "(417, 0.000901003)\n",
      "(418, 0.000898148)\n",
      "(419, 0.000895309)\n",
      "(420, 0.000892484)\n",
      "(421, 0.000889674)\n",
      "(422, 0.000886879)\n",
      "(423, 0.000884099)\n",
      "(424, 0.000881334)\n",
      "(425, 0.000878582)\n",
      "(426, 0.000875846)\n",
      "(427, 0.000873123)\n",
      "(428, 0.000870415)\n",
      "(429, 0.000867721)\n",
      "(430, 0.00086504)\n",
      "(431, 0.000862374)\n",
      "(432, 0.000859721)\n",
      "(433, 0.000857082)\n",
      "(434, 0.000854457)\n",
      "(435, 0.000851845)\n",
      "(436, 0.000849246)\n",
      "(437, 0.000846661)\n",
      "(438, 0.000844088)\n",
      "(439, 0.000841529)\n",
      "(440, 0.000838983)\n",
      "(441, 0.00083645)\n",
      "(442, 0.000833929)\n",
      "(443, 0.000831421)\n",
      "(444, 0.000828926)\n",
      "(445, 0.000826443)\n",
      "(446, 0.000823973)\n",
      "(447, 0.000821515)\n",
      "(448, 0.000819069)\n",
      "(449, 0.000816635)\n",
      "(450, 0.000814214)\n",
      "(451, 0.000811804)\n",
      "(452, 0.000809406)\n",
      "(453, 0.00080702)\n",
      "(454, 0.000804646)\n",
      "(455, 0.000802284)\n",
      "(456, 0.000799933)\n",
      "(457, 0.000797593)\n",
      "(458, 0.000795265)\n",
      "(459, 0.000792949)\n",
      "(460, 0.000790643)\n",
      "(461, 0.000788349)\n",
      "(462, 0.000786066)\n",
      "(463, 0.000783794)\n",
      "(464, 0.000781532)\n",
      "(465, 0.000779282)\n",
      "(466, 0.000777043)\n",
      "(467, 0.000774814)\n",
      "(468, 0.000772596)\n",
      "(469, 0.000770388)\n",
      "(470, 0.000768191)\n",
      "(471, 0.000766004)\n",
      "(472, 0.000763828)\n",
      "(473, 0.000761662)\n",
      "(474, 0.000759506)\n",
      "(475, 0.00075736)\n",
      "(476, 0.000755225)\n",
      "(477, 0.000753099)\n",
      "(478, 0.000750984)\n",
      "(479, 0.000748878)\n",
      "(480, 0.000746782)\n",
      "(481, 0.000744696)\n",
      "(482, 0.000742619)\n",
      "(483, 0.000740552)\n",
      "(484, 0.000738495)\n",
      "(485, 0.000736447)\n",
      "(486, 0.000734409)\n",
      "(487, 0.00073238)\n",
      "(488, 0.00073036)\n",
      "(489, 0.00072835)\n",
      "(490, 0.000726349)\n",
      "(491, 0.000724356)\n",
      "(492, 0.000722373)\n",
      "(493, 0.000720399)\n",
      "(494, 0.000718434)\n",
      "(495, 0.000716478)\n",
      "(496, 0.000714531)\n",
      "(497, 0.000712592)\n",
      "(498, 0.000710662)\n",
      "(499, 0.000708741)\n",
      "(500, 0.000706829)\n",
      "(501, 0.000704925)\n",
      "(502, 0.000703029)\n",
      "(503, 0.000701142)\n",
      "(504, 0.000699264)\n",
      "(505, 0.000697393)\n",
      "(506, 0.000695531)\n",
      "(507, 0.000693677)\n",
      "(508, 0.000691832)\n",
      "(509, 0.000689994)\n",
      "(510, 0.000688165)\n",
      "(511, 0.000686344)\n",
      "(512, 0.00068453)\n",
      "(513, 0.000682725)\n",
      "(514, 0.000680927)\n",
      "(515, 0.000679138)\n",
      "(516, 0.000677356)\n",
      "(517, 0.000675582)\n",
      "(518, 0.000673815)\n",
      "(519, 0.000672056)\n",
      "(520, 0.000670305)\n",
      "(521, 0.000668561)\n",
      "(522, 0.000666825)\n",
      "(523, 0.000665096)\n",
      "(524, 0.000663375)\n",
      "(525, 0.000661661)\n",
      "(526, 0.000659954)\n",
      "(527, 0.000658255)\n",
      "(528, 0.000656563)\n",
      "(529, 0.000654878)\n",
      "(530, 0.0006532)\n",
      "(531, 0.000651529)\n",
      "(532, 0.000649866)\n",
      "(533, 0.000648209)\n",
      "(534, 0.00064656)\n",
      "(535, 0.000644917)\n",
      "(536, 0.000643281)\n",
      "(537, 0.000641652)\n",
      "(538, 0.00064003)\n",
      "(539, 0.000638414)\n",
      "(540, 0.000636806)\n",
      "(541, 0.000635204)\n",
      "(542, 0.000633608)\n",
      "(543, 0.00063202)\n",
      "(544, 0.000630438)\n",
      "(545, 0.000628862)\n",
      "(546, 0.000627293)\n",
      "(547, 0.00062573)\n",
      "(548, 0.000624174)\n",
      "(549, 0.000622624)\n",
      "(550, 0.00062108)\n",
      "(551, 0.000619543)\n",
      "(552, 0.000618012)\n",
      "(553, 0.000616488)\n",
      "(554, 0.000614969)\n",
      "(555, 0.000613457)\n",
      "(556, 0.00061195)\n",
      "(557, 0.00061045)\n",
      "(558, 0.000608956)\n",
      "(559, 0.000607468)\n",
      "(560, 0.000605986)\n",
      "(561, 0.00060451)\n",
      "(562, 0.00060304)\n",
      "(563, 0.000601575)\n",
      "(564, 0.000600117)\n",
      "(565, 0.000598664)\n",
      "(566, 0.000597217)\n",
      "(567, 0.000595776)\n",
      "(568, 0.000594341)\n",
      "(569, 0.000592911)\n",
      "(570, 0.000591487)\n",
      "(571, 0.000590069)\n",
      "(572, 0.000588656)\n",
      "(573, 0.000587248)\n",
      "(574, 0.000585847)\n",
      "(575, 0.00058445)\n",
      "(576, 0.00058306)\n",
      "(577, 0.000581674)\n",
      "(578, 0.000580294)\n",
      "(579, 0.00057892)\n",
      "(580, 0.000577551)\n",
      "(581, 0.000576187)\n",
      "(582, 0.000574828)\n",
      "(583, 0.000573475)\n",
      "(584, 0.000572127)\n",
      "(585, 0.000570784)\n",
      "(586, 0.000569446)\n",
      "(587, 0.000568113)\n",
      "(588, 0.000566786)\n",
      "(589, 0.000565463)\n",
      "(590, 0.000564146)\n",
      "(591, 0.000562834)\n",
      "(592, 0.000561526)\n",
      "(593, 0.000560224)\n",
      "(594, 0.000558927)\n",
      "(595, 0.000557634)\n",
      "(596, 0.000556347)\n",
      "(597, 0.000555064)\n",
      "(598, 0.000553786)\n",
      "(599, 0.000552513)\n",
      "(600, 0.000551245)\n",
      "(601, 0.000549982)\n",
      "(602, 0.000548723)\n",
      "(603, 0.000547469)\n",
      "(604, 0.00054622)\n",
      "(605, 0.000544975)\n",
      "(606, 0.000543735)\n",
      "(607, 0.0005425)\n",
      "(608, 0.000541269)\n",
      "(609, 0.000540043)\n",
      "(610, 0.000538821)\n",
      "(611, 0.000537604)\n",
      "(612, 0.000536391)\n",
      "(613, 0.000535183)\n",
      "(614, 0.00053398)\n",
      "(615, 0.00053278)\n",
      "(616, 0.000531585)\n",
      "(617, 0.000530395)\n",
      "(618, 0.000529209)\n",
      "(619, 0.000528027)\n",
      "(620, 0.00052685)\n",
      "(621, 0.000525676)\n",
      "(622, 0.000524508)\n",
      "(623, 0.000523343)\n",
      "(624, 0.000522182)\n",
      "(625, 0.000521026)\n",
      "(626, 0.000519874)\n",
      "(627, 0.000518726)\n",
      "(628, 0.000517582)\n",
      "(629, 0.000516443)\n",
      "(630, 0.000515307)\n",
      "(631, 0.000514176)\n",
      "(632, 0.000513048)\n",
      "(633, 0.000511925)\n",
      "(634, 0.000510806)\n",
      "(635, 0.00050969)\n",
      "(636, 0.000508579)\n",
      "(637, 0.000507472)\n",
      "(638, 0.000506368)\n",
      "(639, 0.000505268)\n",
      "(640, 0.000504173)\n",
      "(641, 0.000503081)\n",
      "(642, 0.000501993)\n",
      "(643, 0.000500909)\n",
      "(644, 0.000499828)\n",
      "(645, 0.000498752)\n",
      "(646, 0.000497679)\n",
      "(647, 0.00049661)\n",
      "(648, 0.000495545)\n",
      "(649, 0.000494484)\n",
      "(650, 0.000493426)\n",
      "(651, 0.000492372)\n",
      "(652, 0.000491321)\n",
      "(653, 0.000490274)\n",
      "(654, 0.000489231)\n",
      "(655, 0.000488192)\n",
      "(656, 0.000487156)\n",
      "(657, 0.000486123)\n",
      "(658, 0.000485094)\n",
      "(659, 0.000484069)\n",
      "(660, 0.000483047)\n",
      "(661, 0.000482029)\n",
      "(662, 0.000481014)\n",
      "(663, 0.000480003)\n",
      "(664, 0.000478995)\n",
      "(665, 0.000477991)\n",
      "(666, 0.00047699)\n",
      "(667, 0.000475992)\n",
      "(668, 0.000474998)\n",
      "(669, 0.000474007)\n",
      "(670, 0.00047302)\n",
      "(671, 0.000472036)\n",
      "(672, 0.000471055)\n",
      "(673, 0.000470078)\n",
      "(674, 0.000469104)\n",
      "(675, 0.000468133)\n",
      "(676, 0.000467165)\n",
      "(677, 0.000466201)\n",
      "(678, 0.00046524)\n",
      "(679, 0.000464282)\n",
      "(680, 0.000463327)\n",
      "(681, 0.000462375)\n",
      "(682, 0.000461427)\n",
      "(683, 0.000460482)\n",
      "(684, 0.00045954)\n",
      "(685, 0.000458601)\n",
      "(686, 0.000457665)\n",
      "(687, 0.000456732)\n",
      "(688, 0.000455803)\n",
      "(689, 0.000454876)\n",
      "(690, 0.000453953)\n",
      "(691, 0.000453032)\n",
      "(692, 0.000452115)\n",
      "(693, 0.0004512)\n",
      "(694, 0.000450289)\n",
      "(695, 0.000449381)\n",
      "(696, 0.000448475)\n",
      "(697, 0.000447573)\n",
      "(698, 0.000446673)\n",
      "(699, 0.000445776)\n",
      "(700, 0.000444883)\n",
      "(701, 0.000443992)\n",
      "(702, 0.000443104)\n",
      "(703, 0.000442219)\n",
      "(704, 0.000441337)\n",
      "(705, 0.000440457)\n",
      "(706, 0.000439581)\n",
      "(707, 0.000438707)\n",
      "(708, 0.000437836)\n",
      "(709, 0.000436968)\n",
      "(710, 0.000436103)\n",
      "(711, 0.00043524)\n",
      "(712, 0.00043438)\n",
      "(713, 0.000433523)\n",
      "(714, 0.000432669)\n",
      "(715, 0.000431818)\n",
      "(716, 0.000430969)\n",
      "(717, 0.000430123)\n",
      "(718, 0.000429279)\n",
      "(719, 0.000428438)\n",
      "(720, 0.0004276)\n",
      "(721, 0.000426765)\n",
      "(722, 0.000425932)\n",
      "(723, 0.000425102)\n",
      "(724, 0.000424274)\n",
      "(725, 0.000423449)\n",
      "(726, 0.000422627)\n",
      "(727, 0.000421807)\n",
      "(728, 0.000420989)\n",
      "(729, 0.000420175)\n",
      "(730, 0.000419363)\n",
      "(731, 0.000418553)\n",
      "(732, 0.000417746)\n",
      "(733, 0.000416941)\n",
      "(734, 0.000416139)\n",
      "(735, 0.00041534)\n",
      "(736, 0.000414542)\n",
      "(737, 0.000413748)\n",
      "(738, 0.000412956)\n",
      "(739, 0.000412166)\n",
      "(740, 0.000411379)\n",
      "(741, 0.000410594)\n",
      "(742, 0.000409811)\n",
      "(743, 0.000409031)\n",
      "(744, 0.000408253)\n",
      "(745, 0.000407478)\n",
      "(746, 0.000406705)\n",
      "(747, 0.000405935)\n",
      "(748, 0.000405166)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(749, 0.000404401)\n",
      "(750, 0.000403637)\n",
      "(751, 0.000402876)\n",
      "(752, 0.000402117)\n",
      "(753, 0.00040136)\n",
      "(754, 0.000400606)\n",
      "(755, 0.000399854)\n",
      "(756, 0.000399104)\n",
      "(757, 0.000398357)\n",
      "(758, 0.000397612)\n",
      "(759, 0.000396869)\n",
      "(760, 0.000396128)\n",
      "(761, 0.000395389)\n",
      "(762, 0.000394653)\n",
      "(763, 0.000393919)\n",
      "(764, 0.000393187)\n",
      "(765, 0.000392457)\n",
      "(766, 0.00039173)\n",
      "(767, 0.000391004)\n",
      "(768, 0.000390281)\n",
      "(769, 0.00038956)\n",
      "(770, 0.000388841)\n",
      "(771, 0.000388124)\n",
      "(772, 0.00038741)\n",
      "(773, 0.000386697)\n",
      "(774, 0.000385987)\n",
      "(775, 0.000385278)\n",
      "(776, 0.000384572)\n",
      "(777, 0.000383868)\n",
      "(778, 0.000383166)\n",
      "(779, 0.000382466)\n",
      "(780, 0.000381768)\n",
      "(781, 0.000381072)\n",
      "(782, 0.000380378)\n",
      "(783, 0.000379686)\n",
      "(784, 0.000378996)\n",
      "(785, 0.000378308)\n",
      "(786, 0.000377622)\n",
      "(787, 0.000376939)\n",
      "(788, 0.000376257)\n",
      "(789, 0.000375577)\n",
      "(790, 0.000374899)\n",
      "(791, 0.000374223)\n",
      "(792, 0.000373549)\n",
      "(793, 0.000372877)\n",
      "(794, 0.000372207)\n",
      "(795, 0.000371539)\n",
      "(796, 0.000370872)\n",
      "(797, 0.000370208)\n",
      "(798, 0.000369546)\n",
      "(799, 0.000368885)\n",
      "(800, 0.000368226)\n",
      "(801, 0.00036757)\n",
      "(802, 0.000366915)\n",
      "(803, 0.000366262)\n",
      "(804, 0.000365611)\n",
      "(805, 0.000364961)\n",
      "(806, 0.000364314)\n",
      "(807, 0.000363668)\n",
      "(808, 0.000363025)\n",
      "(809, 0.000362383)\n",
      "(810, 0.000361742)\n",
      "(811, 0.000361104)\n",
      "(812, 0.000360468)\n",
      "(813, 0.000359833)\n",
      "(814, 0.0003592)\n",
      "(815, 0.000358569)\n",
      "(816, 0.000357939)\n",
      "(817, 0.000357312)\n",
      "(818, 0.000356686)\n",
      "(819, 0.000356062)\n",
      "(820, 0.000355439)\n",
      "(821, 0.000354819)\n",
      "(822, 0.0003542)\n",
      "(823, 0.000353583)\n",
      "(824, 0.000352967)\n",
      "(825, 0.000352354)\n",
      "(826, 0.000351742)\n",
      "(827, 0.000351131)\n",
      "(828, 0.000350523)\n",
      "(829, 0.000349916)\n",
      "(830, 0.00034931)\n",
      "(831, 0.000348707)\n",
      "(832, 0.000348105)\n",
      "(833, 0.000347505)\n",
      "(834, 0.000346906)\n",
      "(835, 0.000346309)\n",
      "(836, 0.000345714)\n",
      "(837, 0.00034512)\n",
      "(838, 0.000344528)\n",
      "(839, 0.000343937)\n",
      "(840, 0.000343349)\n",
      "(841, 0.000342761)\n",
      "(842, 0.000342176)\n",
      "(843, 0.000341592)\n",
      "(844, 0.000341009)\n",
      "(845, 0.000340429)\n",
      "(846, 0.000339849)\n",
      "(847, 0.000339272)\n",
      "(848, 0.000338695)\n",
      "(849, 0.000338121)\n",
      "(850, 0.000337548)\n",
      "(851, 0.000336976)\n",
      "(852, 0.000336406)\n",
      "(853, 0.000335838)\n",
      "(854, 0.000335271)\n",
      "(855, 0.000334706)\n",
      "(856, 0.000334142)\n",
      "(857, 0.00033358)\n",
      "(858, 0.000333019)\n",
      "(859, 0.00033246)\n",
      "(860, 0.000331902)\n",
      "(861, 0.000331345)\n",
      "(862, 0.000330791)\n",
      "(863, 0.000330237)\n",
      "(864, 0.000329685)\n",
      "(865, 0.000329135)\n",
      "(866, 0.000328586)\n",
      "(867, 0.000328039)\n",
      "(868, 0.000327493)\n",
      "(869, 0.000326948)\n",
      "(870, 0.000326405)\n",
      "(871, 0.000325863)\n",
      "(872, 0.000325323)\n",
      "(873, 0.000324784)\n",
      "(874, 0.000324247)\n",
      "(875, 0.00032371)\n",
      "(876, 0.000323176)\n",
      "(877, 0.000322643)\n",
      "(878, 0.000322111)\n",
      "(879, 0.00032158)\n",
      "(880, 0.000321051)\n",
      "(881, 0.000320524)\n",
      "(882, 0.000319997)\n",
      "(883, 0.000319473)\n",
      "(884, 0.000318949)\n",
      "(885, 0.000318427)\n",
      "(886, 0.000317906)\n",
      "(887, 0.000317387)\n",
      "(888, 0.000316869)\n",
      "(889, 0.000316352)\n",
      "(890, 0.000315836)\n",
      "(891, 0.000315322)\n",
      "(892, 0.000314809)\n",
      "(893, 0.000314298)\n",
      "(894, 0.000313788)\n",
      "(895, 0.000313279)\n",
      "(896, 0.000312772)\n",
      "(897, 0.000312265)\n",
      "(898, 0.000311761)\n",
      "(899, 0.000311257)\n",
      "(900, 0.000310755)\n",
      "(901, 0.000310254)\n",
      "(902, 0.000309754)\n",
      "(903, 0.000309255)\n",
      "(904, 0.000308758)\n",
      "(905, 0.000308262)\n",
      "(906, 0.000307768)\n",
      "(907, 0.000307274)\n",
      "(908, 0.000306782)\n",
      "(909, 0.000306291)\n",
      "(910, 0.000305801)\n",
      "(911, 0.000305313)\n",
      "(912, 0.000304826)\n",
      "(913, 0.00030434)\n",
      "(914, 0.000303855)\n",
      "(915, 0.000303372)\n",
      "(916, 0.000302889)\n",
      "(917, 0.000302408)\n",
      "(918, 0.000301929)\n",
      "(919, 0.00030145)\n",
      "(920, 0.000300972)\n",
      "(921, 0.000300496)\n",
      "(922, 0.000300021)\n",
      "(923, 0.000299547)\n",
      "(924, 0.000299075)\n",
      "(925, 0.000298603)\n",
      "(926, 0.000298133)\n",
      "(927, 0.000297664)\n",
      "(928, 0.000297196)\n",
      "(929, 0.000296729)\n",
      "(930, 0.000296263)\n",
      "(931, 0.000295799)\n",
      "(932, 0.000295335)\n",
      "(933, 0.000294873)\n",
      "(934, 0.000294412)\n",
      "(935, 0.000293952)\n",
      "(936, 0.000293493)\n",
      "(937, 0.000293036)\n",
      "(938, 0.000292579)\n",
      "(939, 0.000292124)\n",
      "(940, 0.000291669)\n",
      "(941, 0.000291216)\n",
      "(942, 0.000290764)\n",
      "(943, 0.000290313)\n",
      "(944, 0.000289863)\n",
      "(945, 0.000289415)\n",
      "(946, 0.000288967)\n",
      "(947, 0.00028852)\n",
      "(948, 0.000288075)\n",
      "(949, 0.00028763)\n",
      "(950, 0.000287187)\n",
      "(951, 0.000286745)\n",
      "(952, 0.000286304)\n",
      "(953, 0.000285864)\n",
      "(954, 0.000285425)\n",
      "(955, 0.000284987)\n",
      "(956, 0.00028455)\n",
      "(957, 0.000284114)\n",
      "(958, 0.000283679)\n",
      "(959, 0.000283246)\n",
      "(960, 0.000282813)\n",
      "(961, 0.000282381)\n",
      "(962, 0.000281951)\n",
      "(963, 0.000281521)\n",
      "(964, 0.000281093)\n",
      "(965, 0.000280665)\n",
      "(966, 0.000280239)\n",
      "(967, 0.000279813)\n",
      "(968, 0.000279389)\n",
      "(969, 0.000278965)\n",
      "(970, 0.000278543)\n",
      "(971, 0.000278121)\n",
      "(972, 0.000277701)\n",
      "(973, 0.000277282)\n",
      "(974, 0.000276863)\n",
      "(975, 0.000276446)\n",
      "(976, 0.000276029)\n",
      "(977, 0.000275614)\n",
      "(978, 0.000275199)\n",
      "(979, 0.000274786)\n",
      "(980, 0.000274374)\n",
      "(981, 0.000273962)\n",
      "(982, 0.000273551)\n",
      "(983, 0.000273142)\n",
      "(984, 0.000272733)\n",
      "(985, 0.000272326)\n",
      "(986, 0.000271919)\n",
      "(987, 0.000271513)\n",
      "(988, 0.000271108)\n",
      "(989, 0.000270705)\n",
      "(990, 0.000270302)\n",
      "(991, 0.0002699)\n",
      "(992, 0.000269499)\n",
      "(993, 0.000269099)\n",
      "(994, 0.0002687)\n",
      "(995, 0.000268301)\n",
      "(996, 0.000267904)\n",
      "(997, 0.000267508)\n",
      "(998, 0.000267112)\n",
      "(999, 0.000266718)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    nn.train([0.05, 0.1], [0.01, 0.99])\n",
    "    print(i, round(nn.calculate_total_error([[[0.05, 0.1], [0.01, 0.99]]]), 9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "* Inputs: 2\n",
      "------\n",
      "Hidden Layer\n",
      "('Neurons:', 2)\n",
      "(' Neuron', 0)\n",
      "('  Weight:', 0.17264682436694792)\n",
      "('  Weight:', 0.24529364873389645)\n",
      "('  Bias:', 0.8029364873389627)\n",
      "(' Neuron', 1)\n",
      "('  Weight:', 0.2719132095965646)\n",
      "('  Weight:', 0.3438264191931294)\n",
      "('  Bias:', 0.8029364873389627)\n",
      "------\n",
      "* Output Layer\n",
      "('Neurons:', 2)\n",
      "(' Neuron', 0)\n",
      "('  Weight:', -1.1875527565945765)\n",
      "('  Weight:', -1.1408368809471354)\n",
      "('  Bias:', -1.9748804737127335)\n",
      "(' Neuron', 1)\n",
      "('  Weight:', 1.2696333683813013)\n",
      "('  Weight:', 1.3205209135783864)\n",
      "('  Bias:', -1.9748804737127335)\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "nn.inspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
